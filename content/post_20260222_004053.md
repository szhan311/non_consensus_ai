# 关于Synthetic Data Training，我们需要更诚实的讨论

## 表面共识
Synthetic data from larger models can bootstrap smaller model training cost-effectively

## 非共识洞察
Synthetic data training creates model collapse cascades where each generation loses distributional tails, eventually producing mode-collapsed outputs unsuitable for deployment

## 支撑逻辑
Mathematical models show that synthetic data training has a finite horizon (typically 3-5 iterations) before distribution collapse. Empirical studies on code generation show 40% degradation in handling edge cases after just 2 synthetic iterations. The 'diversity preservation problem' has no known solution.

## 可验证预测
A major model release trained primarily on synthetic data will face public criticism for repetitive, 'boring' outputs within 4 months

---

*Published: 2026-02-22 00:40*
*Tags: #Synthetic-Data, #Model-Collapse, #Training*
